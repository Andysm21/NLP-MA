{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daBLi64FV88C"
      },
      "source": [
        "# Importing dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puR2ovAMWDUe",
        "outputId": "5bb411c7-0be3-45c7-8a3d-2cf8a896d80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spotify-million-song-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d notshrirang/spotify-million-song-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOpLbsT_XkkV",
        "outputId": "d47c9ae8-f12f-4679-d646-cf40aa45f441"
      },
      "outputs": [],
      "source": [
        "# %unzip spotify-million-song-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fF9qzQIHYupy"
      },
      "outputs": [],
      "source": [
        "spotifyDF = pd.read_csv(\"spotify_millsongdata.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Milestone 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "spotifyDFMS3=spotifyDF.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Ahe's My Kind Of Girl</td>\n",
              "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
              "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
              "      <td>Look at her face, it's a wonderful face And it...</td>\n",
              "      <td>And when we go for a walk in the park And she ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Andante, Andante</td>\n",
              "      <td>/a/abba/andante+andante_20002708.html</td>\n",
              "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
              "      <td>Take it easy with me, please Touch me gently l...</td>\n",
              "      <td>your eyes Like the feeling of a thousand butte...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  artist                   song                                        link  \\\n",
              "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
              "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
              "\n",
              "                                                text  \\\n",
              "0  Look at her face, it's a wonderful face  \\r\\nA...   \n",
              "1  Take it easy with me, please  \\r\\nTouch me gen...   \n",
              "\n",
              "                                                   X  \\\n",
              "0  Look at her face, it's a wonderful face And it...   \n",
              "1  Take it easy with me, please Touch me gently l...   \n",
              "\n",
              "                                                   Y  \n",
              "0  And when we go for a walk in the park And she ...  \n",
              "1  your eyes Like the feeling of a thousand butte...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def split_sentences(df, col_name):\n",
        "    df['X'] = df[col_name].apply(lambda x: ' '.join(x.split()[:len(x.split())//2]))\n",
        "    df['Y'] = df[col_name].apply(lambda x: ' '.join(x.split()[len(x.split())//2:]))\n",
        "    return df\n",
        "\n",
        "spotifyDFMS3=split_sentences(spotifyDFMS3,\"text\")\n",
        "spotifyDFMS3.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "719\n",
            "781\n"
          ]
        }
      ],
      "source": [
        "print(len(spotifyDFMS3[\"Y\"][0])+len(spotifyDFMS3[\"X\"][0]))\n",
        "print(len(spotifyDFMS3[\"text\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a difference in the values since the split_sentences method also removes the extra unneeded tokens such as white spaces adn \\n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "spotifyDFMS3['artist_lyric']=spotifyDFMS3[\"artist\"]+\" || \"+spotifyDFMS3[\"X\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>artist_lyric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Ahe's My Kind Of Girl</td>\n",
              "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
              "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
              "      <td>Look at her face, it's a wonderful face And it...</td>\n",
              "      <td>And when we go for a walk in the park And she ...</td>\n",
              "      <td>ABBA || Look at her face, it's a wonderful fac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Andante, Andante</td>\n",
              "      <td>/a/abba/andante+andante_20002708.html</td>\n",
              "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
              "      <td>Take it easy with me, please Touch me gently l...</td>\n",
              "      <td>your eyes Like the feeling of a thousand butte...</td>\n",
              "      <td>ABBA || Take it easy with me, please Touch me ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  artist                   song                                        link  \\\n",
              "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
              "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
              "\n",
              "                                                text  \\\n",
              "0  Look at her face, it's a wonderful face  \\r\\nA...   \n",
              "1  Take it easy with me, please  \\r\\nTouch me gen...   \n",
              "\n",
              "                                                   X  \\\n",
              "0  Look at her face, it's a wonderful face And it...   \n",
              "1  Take it easy with me, please Touch me gently l...   \n",
              "\n",
              "                                                   Y  \\\n",
              "0  And when we go for a walk in the park And she ...   \n",
              "1  your eyes Like the feeling of a thousand butte...   \n",
              "\n",
              "                                        artist_lyric  \n",
              "0  ABBA || Look at her face, it's a wonderful fac...  \n",
              "1  ABBA || Take it easy with me, please Touch me ...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDFMS3.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X max : 413\n",
            "Y max : 414\n",
            "X AND artist max: 416\n"
          ]
        }
      ],
      "source": [
        "def max_words(df, col_name):\n",
        "    max_word_count = df[col_name].apply(lambda x: len(x.split(\" \"))).max()\n",
        "    return max_word_count\n",
        "\n",
        "max_word_count_X = max_words(spotifyDFMS3, 'X')\n",
        "max_word_count_Y = max_words(spotifyDFMS3, 'Y')\n",
        "max_word_count = max_words(spotifyDFMS3, 'artist_lyric')\n",
        "print(\"X max :\", max_word_count_X)\n",
        "print(\"Y max :\", max_word_count_Y)\n",
        "print(\"X AND artist max:\",max_word_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "spotifyDataset = load_dataset('csv',data_files=\"spotify_millsongdata.csv\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(spotifyDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_sentences(df,OldDF):\n",
        "    x=[]\n",
        "    y=[]\n",
        "    artist_lyric=[]\n",
        "    for string in OldDF[\"X\"]:\n",
        "        x.append(string)\n",
        "    for string in OldDF[\"Y\"]:\n",
        "        y.append(string)\n",
        "    for string in OldDF[\"artist_lyric\"]:\n",
        "        artist_lyric.append(string)\n",
        "    df = df.add_column(\"X\", x)\n",
        "    df = df.add_column(\"Y\", y)\n",
        "    df = df.add_column(\"artist_lyric\", artist_lyric)\n",
        "    return df\n",
        "\n",
        "spotifyDataset=split_sentences(spotifyDataset,spotifyDFMS3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'artist': 'ABBA',\n",
              " 'song': \"Ahe's My Kind Of Girl\",\n",
              " 'link': '/a/abba/ahes+my+kind+of+girl_20598417.html',\n",
              " 'text': \"Look at her face, it's a wonderful face  \\r\\nAnd it means something special to me  \\r\\nLook at the way that she smiles when she sees me  \\r\\nHow lucky can one fellow be?  \\r\\n  \\r\\nShe's just my kind of girl, she makes me feel fine  \\r\\nWho could ever believe that she could be mine?  \\r\\nShe's just my kind of girl, without her I'm blue  \\r\\nAnd if she ever leaves me what could I do, what could I do?  \\r\\n  \\r\\nAnd when we go for a walk in the park  \\r\\nAnd she holds me and squeezes my hand  \\r\\nWe'll go on walking for hours and talking  \\r\\nAbout all the things that we plan  \\r\\n  \\r\\nShe's just my kind of girl, she makes me feel fine  \\r\\nWho could ever believe that she could be mine?  \\r\\nShe's just my kind of girl, without her I'm blue  \\r\\nAnd if she ever leaves me what could I do, what could I do?\\r\\n\\r\\n\",\n",
              " 'X': \"Look at her face, it's a wonderful face And it means something special to me Look at the way that she smiles when she sees me How lucky can one fellow be? She's just my kind of girl, she makes me feel fine Who could ever believe that she could be mine? She's just my kind of girl, without her I'm blue And if she ever leaves me what could I do, what could I do?\",\n",
              " 'Y': \"And when we go for a walk in the park And she holds me and squeezes my hand We'll go on walking for hours and talking About all the things that we plan She's just my kind of girl, she makes me feel fine Who could ever believe that she could be mine? She's just my kind of girl, without her I'm blue And if she ever leaves me what could I do, what could I do?\",\n",
              " 'artist_lyric': \"ABBA || Look at her face, it's a wonderful face And it means something special to me Look at the way that she smiles when she sees me How lucky can one fellow be? She's just my kind of girl, she makes me feel fine Who could ever believe that she could be mine? She's just my kind of girl, without her I'm blue And if she ever leaves me what could I do, what could I do?\"}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_drop = [\"song\", \"link\",\"text\"]\n",
        "spotifyDataset = spotifyDataset.remove_columns(columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'artist': 'ABBA',\n",
              " 'X': \"Look at her face, it's a wonderful face And it means something special to me Look at the way that she smiles when she sees me How lucky can one fellow be? She's just my kind of girl, she makes me feel fine Who could ever believe that she could be mine? She's just my kind of girl, without her I'm blue And if she ever leaves me what could I do, what could I do?\",\n",
              " 'Y': \"And when we go for a walk in the park And she holds me and squeezes my hand We'll go on walking for hours and talking About all the things that we plan She's just my kind of girl, she makes me feel fine Who could ever believe that she could be mine? She's just my kind of girl, without her I'm blue And if she ever leaves me what could I do, what could I do?\",\n",
              " 'artist_lyric': \"ABBA || Look at her face, it's a wonderful face And it means something special to me Look at the way that she smiles when she sees me How lucky can one fellow be? She's just my kind of girl, she makes me feel fine Who could ever believe that she could be mine? She's just my kind of girl, without her I'm blue And if she ever leaves me what could I do, what could I do?\"}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['artist', 'X', 'Y', 'artist_lyric'],\n",
              "    num_rows: 57650\n",
              "})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataset(example):\n",
        "    artist = example['artist']\n",
        "    first_half = example['X']\n",
        "    second_half = example['Y']\n",
        "    \n",
        "    input_text = f\"{artist}: {first_half}\"\n",
        "    target_text = second_half\n",
        "    \n",
        "    return {'input_text': input_text, 'target_text': target_text}\n",
        "\n",
        "spotifyDataset = spotifyDataset.map(process_dataset, remove_columns=[\"X\", \"Y\", \"artist_lyric\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['artist', 'input_text', 'target_text'],\n",
              "    num_rows: 57650\n",
              "})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 57650/57650 [00:24<00:00, 2372.02 examples/s]\n"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    inputs = [f\"<s> {text} </s>\" for text in examples['input_text']]\n",
        "    targets = [f\"<s> {text} </s>\" for text in examples['target_text']]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_word_count, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=max_word_count, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    decoder_input_ids = labels['input_ids'].copy()\n",
        "    decoder_start_token_id = tokenizer.pad_token_id\n",
        "    \n",
        "    for idx in range(len(decoder_input_ids)):\n",
        "        decoder_input_ids[idx] = [decoder_start_token_id] + decoder_input_ids[idx][:-1]\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "spotifyDataset = spotifyDataset.map(tokenize_function, batched=True, remove_columns=[\"artist\", \"input_text\", \"target_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "    num_rows: 57650\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(spotifyDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "spotifyDataset=spotifyDataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "        num_rows: 46120\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "        num_rows: 11530\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = spotifyDataset[\"train\"].with_format(\"numpy\")[:] \n",
        "test_dataset = spotifyDataset[\"test\"].with_format(\"numpy\")[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x0000025B607B3920> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function infer_framework at 0x0000025B607B3920> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:From c:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            " 8/24 [=========>....................] - ETA: 58:20 - loss: 10.3900 - sparse_categorical_accuracy: 0.1490  "
          ]
        }
      ],
      "source": [
        "model.fit(train_dataset, validation_data=test_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "\n",
        "# optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def convert_to_tf_dataset(hf_dataset):\n",
        "#     return hf_dataset.to_tf_dataset(\n",
        "#         columns=['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
        "#         shuffle=True,\n",
        "#     )\n",
        "\n",
        "# train_dataset = convert_to_tf_dataset(spotifyDataset['train'])\n",
        "# eval_dataset = convert_to_tf_dataset(spotifyDataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def train_step(batch):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], decoder_input_ids=batch['decoder_input_ids'], labels=batch['labels'])\n",
        "#         loss = outputs.loss\n",
        "#         logits = outputs.logits\n",
        "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
        "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "#     accuracy_metric.update_state(batch['labels'], logits)\n",
        "    \n",
        "#     return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def eval_step(batch):\n",
        "#     outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], decoder_input_ids=batch['decoder_input_ids'], labels=batch['labels'])\n",
        "#     loss = outputs.loss\n",
        "#     logits = outputs.logits\n",
        "#     accuracy_metric.update_state(batch['labels'], logits)\n",
        "#     return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# num_epochs = 3\n",
        "# train_steps = len(train_dataset)\n",
        "# eval_steps = len(eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for epoch in range(num_epochs):\n",
        "#     # Training\n",
        "#     print('TRAINING')\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "#     total_loss = 0.0\n",
        "#     accuracy_metric.reset_states()\n",
        "#     for step, batch in enumerate(train_dataset):\n",
        "#         loss = train_step(batch)\n",
        "#         total_loss += tf.reduce_mean(loss).numpy()\n",
        "#         if (step + 1) % 100 == 0:\n",
        "#             print(f\"Step {step+1}/{train_steps}, Loss: {total_loss / (step+1):.4f}, Accuracy: {accuracy_metric.result().numpy():.4f}\")\n",
        "    \n",
        "#     # Evaluation\n",
        "#     print('EVALUATION')\n",
        "#     total_loss = 0.0\n",
        "#     accuracy_metric.reset_states()\n",
        "#     for step, batch in enumerate(eval_dataset):\n",
        "#         loss = eval_step(batch)\n",
        "#         total_loss += tf.reduce_mean(loss).numpy()\n",
        "    \n",
        "#     print(f\"Validation Loss: {total_loss / eval_steps:.4f}, Validation Accuracy: {accuracy_metric.result().numpy():.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
