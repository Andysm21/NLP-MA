{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daBLi64FV88C"
      },
      "source": [
        "# Importing dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puR2ovAMWDUe",
        "outputId": "5bb411c7-0be3-45c7-8a3d-2cf8a896d80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spotify-million-song-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d notshrirang/spotify-million-song-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOpLbsT_XkkV",
        "outputId": "d47c9ae8-f12f-4679-d646-cf40aa45f441"
      },
      "outputs": [],
      "source": [
        "# %unzip spotify-million-song-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fF9qzQIHYupy"
      },
      "outputs": [],
      "source": [
        "spotifyDF = pd.read_csv(\"spotifyDFMS3.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Milestone 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From e:\\Python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From e:\\Python\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "spotifyDFMS3=spotifyDF.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bob Dylan</td>\n",
              "      <td>4Th Time Around</td>\n",
              "      <td>/b/bob+dylan/4th+time+around_20021421.html</td>\n",
              "      <td>When she said, \"Don't waste your words, they'r...</td>\n",
              "      <td>When she said, \"Don't waste your words, they'r...</td>\n",
              "      <td>I waited in the hallway, she went to get it, A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bob Dylan</td>\n",
              "      <td>A Satisfied Mind</td>\n",
              "      <td>/b/bob+dylan/a+satisfied+mind_20021501.html</td>\n",
              "      <td>How many times have you heard someone say  \\r\\...</td>\n",
              "      <td>How many times have you heard someone say If I...</td>\n",
              "      <td>suddenly it happened Hmm, I lost every dime Bu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      artist              song                                         link  \\\n",
              "0  Bob Dylan   4Th Time Around   /b/bob+dylan/4th+time+around_20021421.html   \n",
              "1  Bob Dylan  A Satisfied Mind  /b/bob+dylan/a+satisfied+mind_20021501.html   \n",
              "\n",
              "                                                text  \\\n",
              "0  When she said, \"Don't waste your words, they'r...   \n",
              "1  How many times have you heard someone say  \\r\\...   \n",
              "\n",
              "                                                   X  \\\n",
              "0  When she said, \"Don't waste your words, they'r...   \n",
              "1  How many times have you heard someone say If I...   \n",
              "\n",
              "                                                   Y  \n",
              "0  I waited in the hallway, she went to get it, A...  \n",
              "1  suddenly it happened Hmm, I lost every dime Bu...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def split_sentences(df, col_name):\n",
        "    df['X'] = df[col_name].apply(lambda x: ' '.join(x.split()[:len(x.split())//2]))\n",
        "    df['Y'] = df[col_name].apply(lambda x: ' '.join(x.split()[len(x.split())//2:]))\n",
        "    return df\n",
        "\n",
        "spotifyDFMS3=split_sentences(spotifyDFMS3,\"text\")\n",
        "spotifyDFMS3.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1338\n",
            "1482\n"
          ]
        }
      ],
      "source": [
        "print(len(spotifyDFMS3[\"Y\"][0])+len(spotifyDFMS3[\"X\"][0]))\n",
        "print(len(spotifyDFMS3[\"text\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a difference in the values since the split_sentences method also removes the extra unneeded tokens such as white spaces adn \\n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "spotifyDFMS3['artist_lyric']=spotifyDFMS3[\"artist\"]+\" || \"+spotifyDFMS3[\"X\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>artist_lyric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bob Dylan</td>\n",
              "      <td>4Th Time Around</td>\n",
              "      <td>/b/bob+dylan/4th+time+around_20021421.html</td>\n",
              "      <td>When she said, \"Don't waste your words, they'r...</td>\n",
              "      <td>When she said, \"Don't waste your words, they'r...</td>\n",
              "      <td>I waited in the hallway, she went to get it, A...</td>\n",
              "      <td>Bob Dylan || When she said, \"Don't waste your ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bob Dylan</td>\n",
              "      <td>A Satisfied Mind</td>\n",
              "      <td>/b/bob+dylan/a+satisfied+mind_20021501.html</td>\n",
              "      <td>How many times have you heard someone say  \\r\\...</td>\n",
              "      <td>How many times have you heard someone say If I...</td>\n",
              "      <td>suddenly it happened Hmm, I lost every dime Bu...</td>\n",
              "      <td>Bob Dylan || How many times have you heard som...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      artist              song                                         link  \\\n",
              "0  Bob Dylan   4Th Time Around   /b/bob+dylan/4th+time+around_20021421.html   \n",
              "1  Bob Dylan  A Satisfied Mind  /b/bob+dylan/a+satisfied+mind_20021501.html   \n",
              "\n",
              "                                                text  \\\n",
              "0  When she said, \"Don't waste your words, they'r...   \n",
              "1  How many times have you heard someone say  \\r\\...   \n",
              "\n",
              "                                                   X  \\\n",
              "0  When she said, \"Don't waste your words, they'r...   \n",
              "1  How many times have you heard someone say If I...   \n",
              "\n",
              "                                                   Y  \\\n",
              "0  I waited in the hallway, she went to get it, A...   \n",
              "1  suddenly it happened Hmm, I lost every dime Bu...   \n",
              "\n",
              "                                        artist_lyric  \n",
              "0  Bob Dylan || When she said, \"Don't waste your ...  \n",
              "1  Bob Dylan || How many times have you heard som...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDFMS3.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X max : 396\n",
            "Y max : 397\n",
            "X AND artist max: 399\n"
          ]
        }
      ],
      "source": [
        "def max_words(df, col_name):\n",
        "    max_word_count = df[col_name].apply(lambda x: len(x.split(\" \"))).max()\n",
        "    return max_word_count\n",
        "\n",
        "max_word_count_X = max_words(spotifyDFMS3, 'X')\n",
        "max_word_count_Y = max_words(spotifyDFMS3, 'Y')\n",
        "max_word_count = max_words(spotifyDFMS3, 'artist_lyric')\n",
        "print(\"X max :\", max_word_count_X)\n",
        "print(\"Y max :\", max_word_count_Y)\n",
        "print(\"X AND artist max:\",max_word_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "265cb0323b5147a8a87b56cfcbe81255",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "spotifyDataset = load_dataset('csv',data_files=\"spotifyDFMS3.csv\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(spotifyDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_sentences(df,OldDF):\n",
        "    x=[]\n",
        "    y=[]\n",
        "    artist_lyric=[]\n",
        "    for string in OldDF[\"X\"]:\n",
        "        x.append(string)\n",
        "    for string in OldDF[\"Y\"]:\n",
        "        y.append(string)\n",
        "    for string in OldDF[\"artist_lyric\"]:\n",
        "        artist_lyric.append(string)\n",
        "    df = df.add_column(\"X\", x)\n",
        "    df = df.add_column(\"Y\", y)\n",
        "    df = df.add_column(\"artist_lyric\", artist_lyric)\n",
        "    return df\n",
        "\n",
        "spotifyDataset=split_sentences(spotifyDataset,spotifyDFMS3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'artist': 'Bob Dylan',\n",
              " 'song': '4Th Time Around',\n",
              " 'link': '/b/bob+dylan/4th+time+around_20021421.html',\n",
              " 'text': 'When she said, \"Don\\'t waste your words, they\\'re just lies,\"  \\r\\nI cried she was deaf.  \\r\\nAnd she worked on my face until breaking my eyes,  \\r\\nThen said, \"What else you got left?\"  \\r\\nIt was then that I got up to leave  \\r\\nBut she said, \"Don\\'t forget,  \\r\\nEverybody must give something back  \\r\\nFor something they get.\"  \\r\\n  \\r\\nI stood there and hummed,  \\r\\nI tapped on her drum  \\r\\nAnd asked her how come.  \\r\\nAnd she buttoned her boot,  \\r\\nAnd straightened her suit,  \\r\\nThen she said, \"Don\\'t get cute.\"  \\r\\nSo I forced my hands in my pockets  \\r\\nAnd felt with my thumbs,  \\r\\nAnd gallantly handed her  \\r\\nMy very last piece of gum.  \\r\\n  \\r\\nShe threw me outside,  \\r\\nI stood in the dirt where ev\\'ryone walked.  \\r\\nAnd after finding I\\'d forgotten my shirt,  \\r\\nI went back and knocked.  \\r\\nI waited in the hallway, she went to get it,  \\r\\nAnd I tried to make sense  \\r\\nOut of that picture of you in your wheelchair  \\r\\nThat leaned up against--  \\r\\n  \\r\\nHer Jamaican rum  \\r\\nAnd when she did come, I asked her for some.  \\r\\nShe said, \"No, dear.\"  \\r\\nI said, \"Your words aren\\'t clear,  \\r\\nYou\\'d better spit out your gum.\"  \\r\\nShe screamed till her face got so red,  \\r\\nThen she fell on the floor,  \\r\\nAnd I covered her up and then  \\r\\nThought I\\'d go look through her drawer.  \\r\\n  \\r\\nAnd when I was through  \\r\\nI filled up my shoe and brought it to you.  \\r\\nAnd you, you took me in,  \\r\\nYou loved me then, you never wasted time.  \\r\\nAnd I, I never took much,  \\r\\nI never asked for your crutch  \\r\\nAnd I don\\'t ask for mine.\\r\\n\\r\\n',\n",
              " 'X': 'When she said, \"Don\\'t waste your words, they\\'re just lies,\" I cried she was deaf. And she worked on my face until breaking my eyes, Then said, \"What else you got left?\" It was then that I got up to leave But she said, \"Don\\'t forget, Everybody must give something back For something they get.\" I stood there and hummed, I tapped on her drum And asked her how come. And she buttoned her boot, And straightened her suit, Then she said, \"Don\\'t get cute.\" So I forced my hands in my pockets And felt with my thumbs, And gallantly handed her My very last piece of gum. She threw me outside, I stood in the dirt where ev\\'ryone walked. And after finding I\\'d forgotten my shirt, I went back and knocked.',\n",
              " 'Y': 'I waited in the hallway, she went to get it, And I tried to make sense Out of that picture of you in your wheelchair That leaned up against-- Her Jamaican rum And when she did come, I asked her for some. She said, \"No, dear.\" I said, \"Your words aren\\'t clear, You\\'d better spit out your gum.\" She screamed till her face got so red, Then she fell on the floor, And I covered her up and then Thought I\\'d go look through her drawer. And when I was through I filled up my shoe and brought it to you. And you, you took me in, You loved me then, you never wasted time. And I, I never took much, I never asked for your crutch And I don\\'t ask for mine.',\n",
              " 'artist_lyric': 'Bob Dylan || When she said, \"Don\\'t waste your words, they\\'re just lies,\" I cried she was deaf. And she worked on my face until breaking my eyes, Then said, \"What else you got left?\" It was then that I got up to leave But she said, \"Don\\'t forget, Everybody must give something back For something they get.\" I stood there and hummed, I tapped on her drum And asked her how come. And she buttoned her boot, And straightened her suit, Then she said, \"Don\\'t get cute.\" So I forced my hands in my pockets And felt with my thumbs, And gallantly handed her My very last piece of gum. She threw me outside, I stood in the dirt where ev\\'ryone walked. And after finding I\\'d forgotten my shirt, I went back and knocked.'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_drop = [\"song\", \"link\",\"text\"]\n",
        "spotifyDataset = spotifyDataset.remove_columns(columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'artist': 'Bob Dylan',\n",
              " 'X': 'When she said, \"Don\\'t waste your words, they\\'re just lies,\" I cried she was deaf. And she worked on my face until breaking my eyes, Then said, \"What else you got left?\" It was then that I got up to leave But she said, \"Don\\'t forget, Everybody must give something back For something they get.\" I stood there and hummed, I tapped on her drum And asked her how come. And she buttoned her boot, And straightened her suit, Then she said, \"Don\\'t get cute.\" So I forced my hands in my pockets And felt with my thumbs, And gallantly handed her My very last piece of gum. She threw me outside, I stood in the dirt where ev\\'ryone walked. And after finding I\\'d forgotten my shirt, I went back and knocked.',\n",
              " 'Y': 'I waited in the hallway, she went to get it, And I tried to make sense Out of that picture of you in your wheelchair That leaned up against-- Her Jamaican rum And when she did come, I asked her for some. She said, \"No, dear.\" I said, \"Your words aren\\'t clear, You\\'d better spit out your gum.\" She screamed till her face got so red, Then she fell on the floor, And I covered her up and then Thought I\\'d go look through her drawer. And when I was through I filled up my shoe and brought it to you. And you, you took me in, You loved me then, you never wasted time. And I, I never took much, I never asked for your crutch And I don\\'t ask for mine.',\n",
              " 'artist_lyric': 'Bob Dylan || When she said, \"Don\\'t waste your words, they\\'re just lies,\" I cried she was deaf. And she worked on my face until breaking my eyes, Then said, \"What else you got left?\" It was then that I got up to leave But she said, \"Don\\'t forget, Everybody must give something back For something they get.\" I stood there and hummed, I tapped on her drum And asked her how come. And she buttoned her boot, And straightened her suit, Then she said, \"Don\\'t get cute.\" So I forced my hands in my pockets And felt with my thumbs, And gallantly handed her My very last piece of gum. She threw me outside, I stood in the dirt where ev\\'ryone walked. And after finding I\\'d forgotten my shirt, I went back and knocked.'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['artist', 'X', 'Y', 'artist_lyric'],\n",
              "    num_rows: 943\n",
              "})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fefd60871ef140e1aa35ecde6cca3077",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/943 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def process_dataset(example):\n",
        "    artist = example['artist']\n",
        "    first_half = example['X']\n",
        "    second_half = example['Y']\n",
        "    \n",
        "    input_text = f\"{artist}: {first_half}\"\n",
        "    target_text = second_half\n",
        "    \n",
        "    return {'input_text': input_text, 'target_text': target_text}\n",
        "\n",
        "spotifyDataset = spotifyDataset.map(process_dataset, remove_columns=[\"X\", \"Y\", \"artist_lyric\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['artist', 'input_text', 'target_text'],\n",
              "    num_rows: 943\n",
              "})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc8dc74cb25b497ab2f455031f91d920",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/943 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    inputs = [f\"<s> {text} </s>\" for text in examples['input_text']]\n",
        "    targets = [f\"<s> {text} </s>\" for text in examples['target_text']]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_word_count, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=max_word_count, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    decoder_input_ids = labels['input_ids'].copy()\n",
        "    decoder_start_token_id = tokenizer.pad_token_id\n",
        "    \n",
        "    for idx in range(len(decoder_input_ids)):\n",
        "        decoder_input_ids[idx] = [decoder_start_token_id] + decoder_input_ids[idx][:-1]\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "spotifyDataset = spotifyDataset.map(tokenize_function, batched=True, remove_columns=[\"artist\", \"input_text\", \"target_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "    num_rows: 943\n",
              "})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(spotifyDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "spotifyDataset=spotifyDataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "        num_rows: 754\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "        num_rows: 189\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = spotifyDataset[\"train\"].with_format(\"numpy\")[:] \n",
        "test_dataset = spotifyDataset[\"test\"].with_format(\"numpy\")[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x0000025B607B3920> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function infer_framework at 0x0000025B607B3920> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:From c:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            " 8/24 [=========>....................] - ETA: 58:20 - loss: 10.3900 - sparse_categorical_accuracy: 0.1490  "
          ]
        }
      ],
      "source": [
        "# model.fit(train_dataset, validation_data=test_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "\n",
        "# optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_tf_dataset(hf_dataset):\n",
        "    return hf_dataset.to_tf_dataset(\n",
        "        columns=['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
        "        shuffle=True,\n",
        "        batch_size=1\n",
        "    )\n",
        "\n",
        "train_dataset = convert_to_tf_dataset(spotifyDataset['train'])\n",
        "eval_dataset = convert_to_tf_dataset(spotifyDataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_step(batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], decoder_input_ids=batch['decoder_input_ids'], labels=batch['labels'])\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    accuracy_metric.update_state(batch['labels'], logits)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_step(batch):\n",
        "    outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], decoder_input_ids=batch['decoder_input_ids'], labels=batch['labels'])\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "    accuracy_metric.update_state(batch['labels'], logits)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 3\n",
        "train_steps = len(train_dataset)\n",
        "eval_steps = len(eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING\n",
            "Epoch 1/3\n",
            "Step 100/754, Loss: 2.9951, Accuracy: 0.6227\n",
            "Step 200/754, Loss: 2.0151, Accuracy: 0.7241\n",
            "Step 300/754, Loss: 1.6826, Accuracy: 0.7564\n",
            "Step 400/754, Loss: 1.5234, Accuracy: 0.7707\n",
            "Step 500/754, Loss: 1.4276, Accuracy: 0.7794\n",
            "Step 600/754, Loss: 1.3637, Accuracy: 0.7860\n",
            "Step 700/754, Loss: 1.3252, Accuracy: 0.7892\n",
            "EVALUATION\n",
            "Validation Loss: 0.9789, Validation Accuracy: 0.8265\n",
            "TRAINING\n",
            "Epoch 2/3\n",
            "Step 100/754, Loss: 0.9598, Accuracy: 0.8302\n",
            "Step 200/754, Loss: 0.9571, Accuracy: 0.8305\n",
            "Step 300/754, Loss: 0.9486, Accuracy: 0.8316\n",
            "Step 400/754, Loss: 0.9558, Accuracy: 0.8298\n",
            "Step 500/754, Loss: 0.9559, Accuracy: 0.8300\n",
            "Step 600/754, Loss: 0.9609, Accuracy: 0.8292\n",
            "Step 700/754, Loss: 0.9592, Accuracy: 0.8291\n",
            "EVALUATION\n",
            "Validation Loss: 0.9464, Validation Accuracy: 0.8313\n",
            "TRAINING\n",
            "Epoch 3/3\n",
            "Step 100/754, Loss: 0.9786, Accuracy: 0.8276\n",
            "Step 200/754, Loss: 0.8887, Accuracy: 0.8418\n",
            "Step 300/754, Loss: 0.8892, Accuracy: 0.8399\n",
            "Step 400/754, Loss: 0.8881, Accuracy: 0.8404\n",
            "Step 500/754, Loss: 0.9013, Accuracy: 0.8378\n",
            "Step 600/754, Loss: 0.9045, Accuracy: 0.8372\n",
            "Step 700/754, Loss: 0.9134, Accuracy: 0.8352\n",
            "EVALUATION\n",
            "Validation Loss: 0.9322, Validation Accuracy: 0.8326\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    print('TRAINING')\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    total_loss = 0.0\n",
        "    accuracy_metric.reset_states()\n",
        "    for step, batch in enumerate(train_dataset):\n",
        "        loss = train_step(batch)\n",
        "        total_loss += tf.reduce_mean(loss).numpy()\n",
        "        if (step + 1) % 100 == 0:\n",
        "            print(f\"Step {step+1}/{train_steps}, Loss: {total_loss / (step+1):.4f}, Accuracy: {accuracy_metric.result().numpy():.4f}\")\n",
        "    \n",
        "    # Evaluation\n",
        "    print('EVALUATION')\n",
        "    total_loss = 0.0\n",
        "    accuracy_metric.reset_states()\n",
        "    for step, batch in enumerate(eval_dataset):\n",
        "        loss = eval_step(batch)\n",
        "        total_loss += tf.reduce_mean(loss).numpy()\n",
        "    \n",
        "    print(f\"Validation Loss: {total_loss / eval_steps:.4f}, Validation Accuracy: {accuracy_metric.result().numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "    num_rows: 189\n",
              "})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"How many times have you heard someone say If I had his money I'd do things my way Hmm, but little they know Hmm, it's so hard to find One man in ten with a satisfied mind. Hmm, once I was wading in fortune and fame Everything that I dreamed of to get a start in lifes game But\""
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDFMS3[\"X\"][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"suddenly it happened Hmm, I lost every dime But I'm richer by far with a satisfied mind. Hmm, when my life is over and my time has run out My friends and my loved ones I'll leave there ain't no doubt But one things for certain When it comes my time I'll leave this old world with a satisfied mind.\""
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spotifyDFMS3[\"Y\"][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hmm, I'd live the world and be on it Hmm, but little they know Hmm, it's so hard to find One man in ten with a satisfied mind\""
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(spotifyDFMS3[\"X\"][1],return_tensors=\"tf\").input_ids\n",
        "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
